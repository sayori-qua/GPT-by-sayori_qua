<div align="center">
  <h3>Written by @sayori_qua</h3>
</div>

# English Text Generation with Fine-Tuned GPT-2 ([GPT-by-sayori_qua/generate-en.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/generate-en.py))

This project demonstrates how to fine-tune a GPT-2 model for English text generation using datasets such as **OpenWebText and BookCorpus**. The model is trained to continue prompts in a natural and coherent way.

**üìù Overview**

The main goal of this project is to:

Load and preprocess large-scale datasets (OpenWebText and BookCorpus).
Fine-tune a GPT-2 model for causal language modeling.
Generate high-quality English text continuations based on input prompts.

**üß∞ Requirements**

Make sure you have the following installed:
```bash
pip install torch transformers datasets nltk bs4 langdetect rouge sacrebleu pandas tqdm joblib tensorboard
```
Also, download required NLTK resources:
```bash
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

**üß™ Prompts for Testing**

```bash
prompts_continues = [
    'I can', 
    'I wonder', 
    'He always met her', 
    'Gym is very good because',
    'She stood by the window watching...',
    ...
]
```

**Training Process:**

1. Tokenizes and pre-processes data (HTML cleaning, URL removal, stopwords filtering).
2. Splits data into train/test sets.
3. Trains the model using Hugging Face's Trainer.
4. Saves the best checkpoint locally.

**üß† Text Generation**

After training, the model generates text continuations using advanced decoding strategies like **beam search, top-k sampling, and repetition penalty.**

**üß† Sample Output**

```none
Prompt: Gym is very good because
Generated text: Gym is very good because it allows you to take your time and get things done in a short amount of time so you don t have to worry about how long it takes you.
``` 
# Russian Text Generation with Fine-Tuned GPT Model ([GPT-by-sayori_qua/generate-ru.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/generate-ru.py))

This project demonstrates how to fine-tune a Russian GPT model: **IlyaGusev/rugpt3medium_sum_gazeta** on the RUwiki dataset for Russian text generation. After training, the model is capable of continuing prompts in natural and coherent Russian.

**üß∞ Requirements**

```bash
pip install torch transformers datasets nltk bs4 langdetect rouge sacrebleu pandas tqdm joblib tensorboard 
```
Also, download required NLTK resources:

```bash
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

**üß™ Prompts for Testing**

```bash
Some example prompts used for testing the model:
prompts_continues = [
    '–Ø –ª—é–±–ª—é –≥—É–ª—è—Ç—å –ø–æ –≤–µ—á–µ—Ä–∞–º',
    '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - —ç—Ç–æ',
    '–ë–æ—Ä—è –∑–∞–≤—Ç—Ä–∞ –∏–¥–µ—Ç –≤ —à–∫–æ–ª—É, –∞ —è',
    '–û–Ω–∞ –≤—Å—Ç–∞–ª–∞ —É –æ–∫–Ω–∞, –≥–ª—è–¥—è –Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—â–∏—Ö –ª—é–¥–µ–π...',
    ...
]
```

**Training Process**

The pipeline includes:
1. Loading and preprocessing Russian texts (HTML cleaning, URL removal, punctuation handling).
2. Splitting data into train/test sets.
3. Tokenizing using the Russian GPT tokenizer.
4. Training the model using Hugging Face's Trainer.
5. Saving the best checkpoint locally.

**üß† Text Generation**

After training, the model generates continuations using advanced decoding strategies such as **beam search, top-k sampling, and repetition penalty.**

**üß† Sample Output**

```bash
Prompt: –û–Ω–∞ –≤—Å—Ç–∞–ª–∞ —É –æ–∫–Ω–∞, –≥–ª—è–¥—è –Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—â–∏—Ö –ª—é–¥–µ–π, –º–∞—à–∏–Ω—ã, –ª–∏—Å—Ç—å—è, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ç–µ—Ä
Generated text: –û–Ω–∞ –≤—Å—Ç–∞–ª–∞ —É –æ–∫–Ω–∞, –≥–ª—è–¥—è –Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—â–∏—Ö –ª—é–¥–µ–π, –º–∞—à–∏–Ω—ã, –ª–∏—Å—Ç—å—è, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ç–µ—Ä —Å—Ä—ã–≤–∞–ª —Å –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –±—Ä–æ—Å–∞–ª –≤ –Ω–µ–±–æ. –û–Ω–∞ –Ω–µ –≤–∏–¥–µ–ª–∞ –Ω–∏—á–µ–≥–æ, –∫—Ä–æ–º–µ —Ç–æ–≥–æ, —á—Ç–æ –±—ã–ª–æ –ø–µ—Ä–µ–¥ –µ–µ –≥–ª–∞–∑–∞–º–∏, –æ–Ω–∞ –Ω–µ –∑–∞–º–µ—á–∞–ª–∞ –≤—Ä–µ–º–µ–Ω–∏. –£ –Ω–µ—ë –Ω–µ –±—ã–ª–æ –Ω–∏ –ø—Ä–æ—à–ª–æ–≥–æ, –Ω–∏ –±—É–¥—É—â–µ–≥–æ, —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–µ–µ –∏ –Ω–∞—Å—Ç–æ—è—â–µ–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–Ω–∞ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É–≤–∏–¥–∏—Ç. –í —Ç–æ—Ç –¥–µ–Ω—å, –∫–æ–≥–¥–∞ –æ–Ω–∞ —ç—Ç–æ –ø–æ–Ω—è–ª–∞, –∂–∏–∑–Ω—å –µ—ë –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –∏ –æ–Ω–∞ –Ω–∞—á–∞–ª–∞ –Ω–æ–≤—É—é –∂–∏–∑–Ω—å.
```
# English Question Answering Model Training with FLAN-T5 ([GPT-by-sayori_qua/qa-en.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/qa-en.py))

This project demonstrates how to fine-tune a question answering model using the **google/flan-t5-base model** on the **OpenAssistant/oasst1 dataset**. The model is trained to answer user questions in a natural and helpful way.

**üß∞ Requirements**

Make sure you have all required libraries installed:
```bash
pip install torch transformers datasets nltk bs4 langdetect rouge sacrebleu pandas tqdm joblib tensorboard 
```
Also, download NLTK resources:
```bash
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

**üóÇÔ∏è Dataset Used**

OpenAssistant/oasst1 ‚Äî A multilingual dataset of human-AI conversations.
Only high-quality English question-answer pairs are selected based on metadata labels such as:
- Helpfulness
- Toxicity
- Quality
- Creativity

**üß™ Example Prompts**

Some example prompts used for testing:
```bash
prompts_qa_en = [
    "Can you really trust your memory?",
    "Is programming good for the brain?",
    "What happens after we leave this place?",
    "Is it true that a human created you?",
    ...
]
```
You can easily extend or modify these prompts.

**Training Pipeline**

The pipeline includes:

1. Loading and filtering the dataset.
2. Preprocessing text (HTML cleaning, URL removal).
3. Selecting only high-quality English QA pairs.
4. Tokenizing input/output texts.
5. Training the T5 model using Hugging Face's Trainer.
6. Saving the best checkpoint locally.

**üß† Text Generation**

After training, the model generates answers using decoding strategies like:

- Beam search
- Top-p and Top-k sampling
- Repetition penalty

**üß† Sample Output**
```none
Qyestion: Can you really trust your memory?
Answer: Actually... Sure, you can trust your memory. Here are some things to consider when evaluating a memory:
1. The memory is based on information about the person or event that they were thinking about.
2. The memories can vary from person to person depending on the context and context of the event.
3. There are many different types of memories that can be compared to each other.
```

# Russian Question Answering System with Translation ([GPT-by-sayori_qua/qa-ru.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/qa-ru.py))

This project demonstrates how to build a multilingual question answering system , **where a Russian question is translated into English**, answered by an English QA model **google/flan-t5-base**, and the answer **is translated back into Russian**.

**The full pipeline includes:**

1. Language detection
2. Text translation using facebook/nllb-200-distilled-600M
3. QA generation using a fine-tuned FLAN-T5 model

**üß∞ Requirements**

Make sure you have the following libraries installed:
```bash
pip install torch transformers datasets nltk langdetect
```
Also, download NLTK resources:
```bash
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

**üåç Supported Languages**

translation is powered by NLLB-200 tokenizer. You can see all supported languages via:
```bash
translator_tokenizer.lang_code_to_id.keys()
```
Example: 'rus_Cyrl', 'eng_Latn', 'deu_Latn', 'fra_Latn', etc.

**üß™ Example Prompts**

Some example prompts used for testing:
```bash
prompts_qa_ru = [
    "–ú–æ–∂–Ω–æ –ª–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –¥–æ–≤–µ—Ä—è—Ç—å —Å–≤–æ–µ–π –ø–∞–º—è—Ç–∏?",
    "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –º–æ–∑–≥–∞?",
    "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã —É—Ö–æ–¥–∏–º –æ—Ç—Å—é–¥–∞?",
    "–ü—Ä–∞–≤–¥–∞ –ª–∏, —á—Ç–æ —Ç–µ–±—è —Å–æ–∑–¥–∞–ª —á–µ–ª–æ–≤–µ–∫?",
    ...
]
```

**üîÅ Translation Pipeline**

The flow looks like this:

1. Input : Russian question.
2. Language Detection : Detects if input is in Russian or English.
3. Translate to English : Using NLLB model.
4. Generate Answer : Using fine-tuned English QA model.
5. Translate Back to Russian : Final response in native language.

**üß† Sample Output**
```none
Question: –ü–æ—á–µ–º—É –≤—Å—ë —Å—Ç–∞–ª–æ —Ç–∞–∫–∏–º —Å–∫—É—á–Ω—ã–º –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è?
Answer: –í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç—Ç–æ —Å–∫—É—á–Ω–æ –ø–æ –º–Ω–æ–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º:
1. –±—ã–ª–æ –º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã –∏ –Ω—É–∂–Ω–æ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –≤—Å–µ.
2. –ú–Ω–µ —Ç—Ä—É–¥–Ω–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–¥–∞—á–µ, –ø–æ—ç—Ç–æ–º—É —è —á—É–≤—Å—Ç–≤—É—é, —á—Ç–æ –Ω–µ –ø–æ–ª—É—á–∞—é –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è.
3. –ü–æ–≥–æ–¥–∞ —Ç–∞–∫ —Ö–æ–ª–æ–¥–Ω–∞—è, —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç—Ä—É–¥–Ω–æ –∏–¥—Ç–∏ –≤ –Ω–æ–≥—É —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ —Ä–∞–±–æ—Ç—ã.
```
# Multilingual Text Generation with Fine-Tuned GPT Models ([GPT-by-sayori_qua/generate_text_en_ru.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/generate_text_en_ru.py))

This project demonstrates how to use two fine-tuned GPT models for language-specific text generation :

- English model trained on OpenWebText and BookCorpus.
- Russian model trained on RUwiki.
The system automatically detects the input language and uses the appropriate model to generate natural-sounding continuations.

**üß∞ Requirements**

Make sure you have the following libraries installed:
```bash
pip install torch transformers langdetect
```

No additional NLTK or spaCy dependencies are required for this module.

**üîç Language Detection**

The system uses langdetect + custom keyword rules to determine the input language.

**üåç Supported languages:**

- Russian (ru)
- English (en)
Custom keywords ensure better accuracy in cases where language detection fails.

**üß† Text Generation**

Each model is used to continue the user's prompt in its native language:

**English Model**

Model: fine-tuned "openai-community/gpt2"
Postprocessing: Fixes common contractions and capitalization.

**üß† Sample Output**

```none
Prompt: Gym is very good because
Generated text: Gym is very good because it allows you to take your time and get things done in a short amount of time so you don t have to worry about how long it takes you.
``` 

**Russian Model**

Model: IlyaGusev/rugpt3medium_sum_gazeta
Postprocessing: Ensures proper capitalization and punctuation.'

**üß† Sample Output**

```bash
Prompt: –û–Ω–∞ –≤—Å—Ç–∞–ª–∞ —É –æ–∫–Ω–∞, –≥–ª—è–¥—è –Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—â–∏—Ö –ª—é–¥–µ–π, –º–∞—à–∏–Ω—ã, –ª–∏—Å—Ç—å—è, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ç–µ—Ä
Generated text: –û–Ω–∞ –≤—Å—Ç–∞–ª–∞ —É –æ–∫–Ω–∞, –≥–ª—è–¥—è –Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—â–∏—Ö –ª—é–¥–µ–π, –º–∞—à–∏–Ω—ã, –ª–∏—Å—Ç—å—è, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ç–µ—Ä —Å—Ä—ã–≤–∞–ª —Å –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –±—Ä–æ—Å–∞–ª –≤ –Ω–µ–±–æ. –û–Ω–∞ –Ω–µ –≤–∏–¥–µ–ª–∞ –Ω–∏—á–µ–≥–æ, –∫—Ä–æ–º–µ —Ç–æ–≥–æ, —á—Ç–æ –±—ã–ª–æ –ø–µ—Ä–µ–¥ –µ–µ –≥–ª–∞–∑–∞–º–∏, –æ–Ω–∞ –Ω–µ –∑–∞–º–µ—á–∞–ª–∞ –≤—Ä–µ–º–µ–Ω–∏. –£ –Ω–µ—ë –Ω–µ –±—ã–ª–æ –Ω–∏ –ø—Ä–æ—à–ª–æ–≥–æ, –Ω–∏ –±—É–¥—É—â–µ–≥–æ, —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–µ–µ –∏ –Ω–∞—Å—Ç–æ—è—â–µ–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–Ω–∞ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —É–≤–∏–¥–∏—Ç. –í —Ç–æ—Ç –¥–µ–Ω—å, –∫–æ–≥–¥–∞ –æ–Ω–∞ —ç—Ç–æ –ø–æ–Ω—è–ª–∞, –∂–∏–∑–Ω—å –µ—ë –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –∏ –æ–Ω–∞ –Ω–∞—á–∞–ª–∞ –Ω–æ–≤—É—é –∂–∏–∑–Ω—å.
```

# Multilingual Question Answering System ([GPT-by-sayori_qua/qa_txt_voice.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/qa_txt_voice.py))

This project implements a multilingual question answering system that supports both English and Russian . It uses a fine-tuned **FLAN-T5** model for answering questions, and **NLLB** for cross-language translation.

**üß† Features**
- Automatic language detection (Russian/English)
- Translation between languages using facebook/nllb-200-distilled-600M
- Uses a fine-tuned FLAN-T5 model (google/flan-t5-base) for question answering
- Clean response formatting with natural-sounding intros
- Ready to integrate into voice assistants or chatbots

**üß∞ Requirements**
Make sure you have the following installed:
```bash
pip install torch transformers datasets langdetect
```
Optional (for audio processing):
```bash
pip install pydub numpy scipy
```
Also install ffmpeg for audio file handling

**üåç Supported Languages:**

- Input : English (en) or Russian (ru)
- QA Engine : Works on English questions
- Translation : English ‚Üî Russian

**üß™ Example Prompts**
  
Here are some example prompts used in testing:
```bash
prompts_qa_en = [
    "Can you really trust your memory?",
    "Is programming good for the brain?",
    "What happens after we leave this place?",
    "Is it true that a human created you?",
    ...
]
```
**You can also ask questions in Russian:**

```bash
"–ü–æ—á–µ–º—É –≤—Å—ë —Å—Ç–∞–ª–æ —Ç–∞–∫–∏–º —Å–∫—É—á–Ω—ã–º?"
"–ö—Ç–æ —Ç–∞–∫–æ–π –î–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π?"
```

**üîÅ Translation Pipeline**
The flow looks like this:

1. Input : Russian or English question
2. Language Detection : Detects input language
3. Translate to English (if needed)
4. Generate Answer using FLAN-T5
5. Translate Back to Russian (if needed)

**English Model**

Model: **google/flan-t5-base model**

**üß† Sample Output**

```none
Qyestion: Can you really trust your memory?
Answer: Actually... Sure, you can trust your memory. Here are some things to consider when evaluating a memory:
1. The memory is based on information about the person or event that they were thinking about.
2. The memories can vary from person to person depending on the context and context of the event.
3. There are many different types of memories that can be compared to each other.
```

**Russian Model**

Model: **google/flan-t5-base** with translator **facebook/nllb-200-distilled-600M**.

**üß† Sample Output**

```none
Question: –ü–æ—á–µ–º—É –≤—Å—ë —Å—Ç–∞–ª–æ —Ç–∞–∫–∏–º —Å–∫—É—á–Ω—ã–º –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è?
Answer: –í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç—Ç–æ —Å–∫—É—á–Ω–æ –ø–æ –º–Ω–æ–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º:
1. –±—ã–ª–æ –º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã –∏ –Ω—É–∂–Ω–æ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –≤—Å–µ.
2. –ú–Ω–µ —Ç—Ä—É–¥–Ω–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–¥–∞—á–µ, –ø–æ—ç—Ç–æ–º—É —è —á—É–≤—Å—Ç–≤—É—é, —á—Ç–æ –Ω–µ –ø–æ–ª—É—á–∞—é –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è.
3. –ü–æ–≥–æ–¥–∞ —Ç–∞–∫ —Ö–æ–ª–æ–¥–Ω–∞—è, —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç—Ä—É–¥–Ω–æ –∏–¥—Ç–∏ –≤ –Ω–æ–≥—É —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ —Ä–∞–±–æ—Ç—ã.
```

# ü§ñ Telegram QA & Text Generation Bot ([GPT-by-sayori_qua/TelegramBot.py](https://github.com/sayori-qua/GPT-by-sayori_qua/blob/main/TelegramBot.py))
This is a Telegram bot that provides two core functions using AI:

- Text generation ‚Äî the initial phrases (in English or in English) continue.
- Question answering ‚Äî answers text and voice questions (in either language).
The bot uses fine-tuned models for both tasks and supports automatic language detection.

**üîß Features**
- ‚úÖ Multilingual support: English (en) / Russian (ru)
- ‚úÖ /generate ‚Äî Generate text continuation from any prompt
- ‚úÖ /ask ‚Äî Ask a question via text or voice message
- ‚úÖ Voice input recognition using Whisper
- ‚úÖ Language detection + translation under the hood
- ‚úÖ Ready to deploy with your own Hugging Face models

**üß∞ Requirements**
Install dependencies:
```bash
pip install python-telegram-bot pydub numpy torch transformers datasets nltk
```
Also install ffmpeg for voice processing

**üåç Supported Commands:**
- /start ‚Äì Welcome message
- /help ‚Äì Show help
- /generate ‚Äì Start text generation mode
- /ask ‚Äì Ask a question (text or voice)

**üó£Ô∏è Voice Message Flow**
1. User sends voice message
2. Bot converts audio to text using Whisper
3. Detects language automatically
4. Uses QA model to generate answer
5. Sends back response in original language

**üéØHow does this work?**

**English (en):**

- **/generate**
<center>
  <a href="https://postimg.cc/xXNNKHH1"  target="_blank">
    <img src="https://i.postimg.cc/T3CjvJ3g/IMG-8487.jpg"  width="400" alt="Screenshot of /generate command" />
  </a>
  <br />
  <a href="https://postimg.cc/qzPvHFhp"  target="_blank">
    <img src="https://i.postimg.cc/0yQMDR9K/IMG-8488.jpg"  width="400" alt="Screenshot of /ask command" />
  </a>
</center>


- **/ask**
<a href="https://postimg.cc/rKxLxwsL"  target="_blank">
  <img src="https://i.postimg.cc/sD0DFBR1/IMG-8482.jpg"  width="400" alt="Screenshot of /ask command" />
</a>


**Russian (ru):**

- **/generate**
<center>
  <a href="https://postimg.cc/CzHktwtG"  target="_blank">
    <img src="https://i.postimg.cc/TYB0KwJ4/IMG-8485.jpg"  width="400" alt="Screenshot of /generate command" />
  </a>
  <br />
  <a href="https://postimg.cc/QKc1jDMr"  target="_blank">
    <img src="https://i.postimg.cc/6qm0L5JT/IMG-8486.jpg"  width="400" alt="Screenshot of /ask command" />
  </a>
</center>


- **/ask**
<a href="https://postimg.cc/v1N62pY1"  target="_blank">
  <img src="https://i.postimg.cc/7LkNNWc1/IMG-8484.jpg"  width="400" alt="Screenshot of /ask command" />
</a>

